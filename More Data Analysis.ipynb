{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math as m\n",
    "\n",
    "drop_cols = ['month', 'year', 'app_bundle', 'creative_size', 'day_of_week']\n",
    "\n",
    "df = pd.read_csv('csvs/2019-04-00.csv', low_memory=False)\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "for i in range(1,22):\n",
    "    # Read daily csv\n",
    "    temp = pd.read_csv('csvs/2019-04-{}.csv'.format('0'+str(i) if i<10 else str(i)), low_memory=False)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    temp = temp.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # Split daily csv into two dataframes: click and no click\n",
    "    # Note: observations for which clicks = 0 & installs = 1 should actually\n",
    "    # read clicks = 1 & installs = 1\n",
    "    click = temp.loc[(temp.clicks == 1) | (temp.installs == 1)]\n",
    "    click.loc[:, 'clicks'] = 1.0\n",
    "    no_click = temp[(temp.clicks == 0) & (temp.installs == 0)]\n",
    "    \n",
    "    # Downsample no click to be the same size as click\n",
    "    # For reproducibility, set random_state to 0\n",
    "    df = pd.concat([\n",
    "            df, \n",
    "            click,\n",
    "            no_click.sample(n = click.shape[0], random_state = 0)\n",
    "            ], axis=0)\n",
    "    \n",
    "  \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_category=list(df.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MOPUB', 'GOOGLE_ADX', 'PUBMATIC', 'RUBICON']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_inv_source=list(df.inventory_source.unique())\n",
    "L_inv_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sprint',\n",
       " 'T-Mobile',\n",
       " nan,\n",
       " 'AT&T',\n",
       " 'Verizon',\n",
       " 'C-Spire Wireless',\n",
       " 'U.S. Cellular',\n",
       " 'Cellular One',\n",
       " 'Viaero',\n",
       " 'West Central',\n",
       " '-1',\n",
       " 'i wireless',\n",
       " 'Bluegrass Cellular',\n",
       " 'Appalachian Wireless',\n",
       " 'Pioneer Cellular',\n",
       " 'Cincinnati Bell',\n",
       " 'ETEX Wireless']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_platform=list(df.platform_carrier.unique())\n",
    "L_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XL', 'L', nan, 'UNKNOWN', 'M']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_screen_size=list(df.platform_device_screen_size.unique())\n",
    "L_screen_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['MOPUB', 'Sprint', 'IAB14,IAB9,IAB9_30', ..., 0.00337, 1.0, 1.0],\n",
       "       ['MOPUB', 'T-Mobile', 'IAB14,IAB9,IAB9_30', ..., 0.00337, 1.0,\n",
       "        1.0],\n",
       "       ['MOPUB', 'T-Mobile', 'IAB1,IAB9,IAB9_30', ...,\n",
       "        0.006540000000000001, 0.0, 1.0],\n",
       "       ...,\n",
       "       ['MOPUB', 'T-Mobile', 'IAB13,IAB3', ..., 0.00379, 0.0, 1.0],\n",
       "       ['MOPUB', 'T-Mobile', 'IAB9,IAB9_30', ..., 0.008239999999999999,\n",
       "        0.0, 1.0],\n",
       "       ['MOPUB', nan, 'IAB9,IAB9_30', ..., 0.008409999999999999, 0.0,\n",
       "        1.0]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[['inventory_source','platform_carrier','category','platform_device_screen_size','bid_floor', 'rewarded','inventory_interstitial']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df['installs'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0.00337, 1.0, 1.0],\n",
       "       [0, 1, 0, ..., 0.00337, 1.0, 1.0],\n",
       "       [0, 1, 1, ..., 0.006540000000000001, 0.0, 1.0],\n",
       "       ...,\n",
       "       [0, 1, 30, ..., 0.00379, 0.0, 1.0],\n",
       "       [0, 1, 2, ..., 0.008239999999999999, 0.0, 1.0],\n",
       "       [0, 2, 2, ..., 0.008409999999999999, 0.0, 1.0]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i][0]=L_inv_source.index(X[i][0])\n",
    "    X[i][1]=L_platform.index(X[i][1])   \n",
    "    X[i][2]=L_category.index(X[i][2])\n",
    "    X[i][3]=L_screen_size.index(X[i][3])\n",
    "            \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2397462, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"csvs/2019-04-22.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['MOPUB', 'AT&T', 'IAB1,IAB9,IAB9_23', ...,\n",
       "        0.00033999999999999997, 0.0, 1.0],\n",
       "       ['MOPUB', nan, 'IAB1,IAB9,IAB9_23', ..., 0.00253, 0.0, 1.0],\n",
       "       ['MOPUB', 'Verizon', 'IAB1,IAB9,IAB9_23', ..., 0.00253, 0.0, 1.0],\n",
       "       ...,\n",
       "       ['MOPUB', nan, 'IAB1,IAB1_6', ..., 7e-05, 0.0, 0.0],\n",
       "       ['MOPUB', 'T-Mobile', 'IAB9,IAB9_23,IAB9_30', ...,\n",
       "        0.0039299999999999995, 0.0, 0.0],\n",
       "       ['MOPUB', 'T-Mobile', 'IAB17,IAB9,IAB9_30', ..., 0.00079, 0.0,\n",
       "        0.0]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=test[['inventory_source','platform_carrier','category','platform_device_screen_size','bid_floor', 'rewarded','inventory_interstitial']].values\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=test['installs'].values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 43, ..., 0.00033999999999999997, 0.0, 1.0],\n",
       "       [0, 2, 43, ..., 0.00253, 0.0, 1.0],\n",
       "       [0, 4, 43, ..., 0.00253, 0.0, 1.0],\n",
       "       ...,\n",
       "       [0, 2, 35, ..., 7e-05, 0.0, 0.0],\n",
       "       [0, 1, 27, ..., 0.0039299999999999995, 0.0, 0.0],\n",
       "       [0, 1, 39, ..., 0.00079, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    \n",
    "    X_test[i][0]=L_inv_source.index(X_test[i][0])\n",
    "    \n",
    "    try:\n",
    "        X_test[i][1]=L_platform.index(X_test[i][1])  \n",
    "    except ValueError:\n",
    "        X_test[i][1]=-1\n",
    "        \n",
    "    try:\n",
    "        X_test[i][2]=L_category.index(X_test[i][2])\n",
    "    except ValueError:\n",
    "        X_test[i][2]=-1\n",
    "        \n",
    "    try:\n",
    "        X_test[i][3]=L_screen_size.index(X_test[i][3])\n",
    "    except ValueError:\n",
    "        X_test[i][3]=-1\n",
    "            \n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    815378\n",
      "         1.0       0.00      0.00      0.00        44\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    815422\n",
      "   macro avg       0.50      0.50      0.50    815422\n",
      "weighted avg       1.00      1.00      1.00    815422\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.5, 0.9993213656775374)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, balanced_accuracy_score\n",
    "\n",
    "LR=LogisticRegression(solver='lbfgs', class_weight={0:1.2,1:1},C=2, max_iter=1000)\n",
    "LR.fit(X,y)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1_score(y_test, y_pred), balanced_accuracy_score(y_test, y_pred),LR.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    815378\n",
      "         1.0       0.00      0.00      0.00        44\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    815422\n",
      "   macro avg       0.50      0.50      0.50    815422\n",
      "weighted avg       1.00      1.00      1.00    815422\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.4999993867874777, 0.9993230341085698)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bc = BaggingClassifier(random_state=10000)\n",
    "\n",
    "bc.fit(X, y) \n",
    "y2 = bc.predict(X_test)\n",
    "print(classification_report(y_test, y2))\n",
    "f1_score(y_test, y2), balanced_accuracy_score(y_test, y2), bc.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    815378\n",
      "         1.0       0.00      0.00      0.00        44\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    815422\n",
      "   macro avg       0.50      0.50      0.50    815422\n",
      "weighted avg       1.00      1.00      1.00    815422\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.5, 0.9993213656775374)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "GBclf = ensemble.GradientBoostingClassifier(learning_rate=0.001, random_state=10000)\n",
    "GBclf.fit(X,y)\n",
    "y3=GBclf.predict(X_test)\n",
    "print(classification_report(y_test, y3))\n",
    "f1_score(y_test, y3), balanced_accuracy_score(y_test, y3),GBclf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
