{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math as m\n",
    "\n",
    "drop_cols = ['month', 'year', 'app_bundle', 'creative_size', 'day_of_week']\n",
    "\n",
    "df = pd.read_csv('csvs/2019-04-00.csv', low_memory=False)\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "for i in range(1,31):\n",
    "    # Read daily csv\n",
    "    temp = pd.read_csv('csvs/2019-04-{}.csv'.format('0'+str(i) if i<10 else str(i)), low_memory=False)\n",
    "    \n",
    "    temp = temp.sample(m.floor(temp.shape[0]/10), random_state = 0)\n",
    "    # Drop unnecessary columns\n",
    "    temp = temp.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # Split daily csv into two dataframes: click and no click\n",
    "    # Note: observations for which clicks = 0 & installs = 1 should actually\n",
    "    # read clicks = 1 & installs = 1\n",
    "    click = temp.loc[(temp.clicks == 1) | (temp.installs == 1)]\n",
    "    click.loc[:, 'clicks'] = 1.0\n",
    "    no_click = temp[(temp.clicks == 0) & (temp.installs == 0)]\n",
    "    \n",
    "    # Downsample no click to be the same size as click\n",
    "    # For reproducibility, set random_state to 0\n",
    "    df = pd.concat([\n",
    "            df, \n",
    "            click,\n",
    "            no_click.sample(n = click.shape[0], random_state = 0)\n",
    "            ], axis=0)\n",
    "    \n",
    "  \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "ZIP = pd.read_csv('zipcode/zipcode.csv')\n",
    "zipdf = ZIP[['zip','state','timezone','dst']]\n",
    "zipdf = zipdf[zipdf.zip > 9999]\n",
    "\n",
    "df = df[df.category != '-1']\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.merge(\n",
    "        df, \n",
    "        zipdf, \n",
    "        left_on='geo_zip', \n",
    "        right_on='zip',\n",
    "        how='left'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:165: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:175: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:180: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If no corresponding zip is found, set it to be in central time\n",
    "df.loc[df.loc[:,'timezone'].isnull(),'timezone'] = -6\n",
    "df.loc[df.loc[:,'dst'].isnull(),'dst'] = 1\n",
    "df.loc[df.loc[:,'state'].isnull(),'state'] = 'Unknown'\n",
    "\n",
    "# Add column 'local_hour'\n",
    "hours = df['hour']\n",
    "hours_list = hours.tolist()\n",
    "hour_num = list(int(x[0:2]) for x in hours_list)\n",
    "df['local_hour'] = hour_num + df.timezone + df.dst\n",
    "\n",
    "# Keep day and day_of_week consisent with hour\n",
    "day_list = list(df.loc[df.loc[:,'local_hour']<0,'day'])\n",
    "day_list = list(int(x)-1 for x in day_list)\n",
    "df.loc[df.loc[:,'local_hour']<0,'day'] = day_list\n",
    "# Update day\n",
    "day_string = []\n",
    "for x in df.loc[:,'day']:\n",
    "   if x!=0:\n",
    "       day_string.append('April'+ str(x) +',2019')\n",
    "   else:\n",
    "       day_string.append('March 31,2019')\n",
    "dow = list(parser.parse(str).strftime(\"%A\") for str in day_string)\n",
    "df.loc[:,'day_of_week'] = dow\n",
    "df.loc[df.loc[:,'local_hour']<0,'local_hour'] +=24\n",
    "\n",
    "\n",
    "# Identify columns with NA or NaN\n",
    "df.columns[df.isna().any()]\n",
    "\n",
    "# Replace NA/NaN with the string 'NA'\n",
    "df = df.fillna(value = {'category': 'NA',\n",
    "#                        'geo_zip': 'NA',\n",
    "                        'platform_bandwidth': 'NA',\n",
    "                        'platform_carrier': 'NA',\n",
    "                        'platform_device_screen_size': 'NA',\n",
    "                        'creative_type': 'NA'})\n",
    "    \n",
    "# Reformat the category column to be one category per column    \n",
    "expand_category = df['category'].str.split(',', expand = True) \n",
    "expand_category = pd.concat([df.auction_id, expand_category], axis=1)\n",
    "\n",
    "expand_category = pd.melt(expand_category, id_vars = ['auction_id']) \n",
    "\n",
    "expand_category = expand_category[expand_category['value'].notnull()]\n",
    "\n",
    "expand_category = expand_category.pivot_table(\n",
    "        index='auction_id',\n",
    "        columns='value',\n",
    "        aggfunc='size'\n",
    "        )\n",
    "\n",
    "expand_category.columns = ['category_' + str(col) for col in expand_category.columns]\n",
    "expand_category = expand_category.fillna(0)\n",
    "\n",
    "df = pd.merge(\n",
    "        df, \n",
    "        expand_category, \n",
    "        left_on='auction_id', \n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "# Reformat the segment column to be one segment per column    \n",
    "df['segments'] = df['segments'].str.replace(r\"\\[\",\"\")\n",
    "df['segments'] = df['segments'].str.replace(r\"\\]\",\"\")\n",
    "    \n",
    "expand_segment = df['segments'].str.split(', ', expand = True)\n",
    "expand_segment = pd.concat([df.auction_id, expand_segment], axis=1)\n",
    "\n",
    "expand_segment = pd.melt(expand_segment, id_vars = ['auction_id']) \n",
    "\n",
    "expand_segment = expand_segment[expand_segment['value'].notnull()]\n",
    "\n",
    "expand_segment = expand_segment.pivot_table(\n",
    "        index='auction_id',\n",
    "        columns='value',\n",
    "        aggfunc='size'\n",
    "        )\n",
    "\n",
    "expand_segment.columns = ['segment_' + str(col) for col in expand_segment.columns]\n",
    "expand_segment = expand_segment.fillna(0)\n",
    "\n",
    "df = pd.merge(\n",
    "        df, \n",
    "        expand_segment, \n",
    "        left_on='auction_id', \n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "# Save dataset as pickle\n",
    "df.to_pickle('clean_df.pickle')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_pickle('clean_df.pickle')\n",
    "\n",
    "# Create modeling dataset\n",
    "# Drop columns that will not be used in the model\n",
    "df = df.drop(['geo_zip', 'bid_timestamp_utc',\n",
    "              'Unnamed: 0', 'timezone'], axis=1)\n",
    "\n",
    "df_train = df[df.day <= 21]\n",
    "df_test = df[df.day >= 22]\n",
    "\n",
    "# platform_device_model has thousands of values. use the 20 most frequent and\n",
    "# group others into an 'Other' category\n",
    "top_devices = df_train[\n",
    "        'platform_device_model'].value_counts().head(20).index.values.tolist()\n",
    "\n",
    "# group uncommon platform_device_model into one category\n",
    "df_train['platform_device_model'] = np.where(\n",
    "        df_train['platform_device_model'].isin(top_devices),\n",
    "        df_train['platform_device_model'],\n",
    "        'Other')\n",
    "\n",
    "df_test['platform_device_model'] = np.where(\n",
    "        df_test['platform_device_model'].isin(top_devices),\n",
    "        df_test['platform_device_model'],\n",
    "        'Other')\n",
    "\n",
    "# platform_device_make has dozens of values, some very rare. use the 10 most \n",
    "# frequent and group others into an 'Other' category\n",
    "top_makes = df_train[\n",
    "        'platform_device_make'].value_counts().head(10).index.values.tolist()\n",
    "\n",
    "# group uncommon platform_device_model into one category\n",
    "df_train['platform_device_make'] = np.where(\n",
    "        df_train['platform_device_make'].isin(top_makes),\n",
    "        df_train['platform_device_make'],\n",
    "        'Other')\n",
    "\n",
    "df_test['platform_device_make'] = np.where(\n",
    "        df_test['platform_device_make'].isin(top_makes),\n",
    "        df_test['platform_device_make'],\n",
    "        'Other')\n",
    "\n",
    "# Some states are rare- group them into an 'Other' category\n",
    "bottom_states = df[\n",
    "        'state'].value_counts().tail(6).index.values.tolist()\n",
    "\n",
    "# group uncommon platform_device_model into one category\n",
    "df_train['state'] = np.where(\n",
    "        ~df_train['state'].isin(bottom_states),\n",
    "        df_train['state'],\n",
    "        'Other')\n",
    "\n",
    "df_test['state'] = np.where(\n",
    "        ~df_test['state'].isin(bottom_states),\n",
    "        df_test['state'],\n",
    "        'Other')\n",
    "\n",
    "# Some platform_device_screen_size are rare- group them into an 'Other' category\n",
    "bottom_ss = df[\n",
    "        'platform_device_screen_size'].value_counts().tail(3).index.values.tolist()\n",
    "\n",
    "# group uncommon platform_device_model into one category\n",
    "df_train['platform_device_screen_size'] = np.where(\n",
    "        ~df_train['platform_device_screen_size'].isin(bottom_ss),\n",
    "        df_train['platform_device_screen_size'],\n",
    "        'Other')\n",
    "\n",
    "df_test['platform_device_screen_size'] = np.where(\n",
    "        ~df_test['platform_device_screen_size'].isin(bottom_ss),\n",
    "        df_test['platform_device_screen_size'],\n",
    "        'Other')\n",
    "\n",
    "# Some platform_bandwidth are rare- group them into an 'Other' category\n",
    "bottom_band = df[\n",
    "        'platform_bandwidth'].value_counts().tail(4).index.values.tolist()\n",
    "\n",
    "# group uncommon platform_device_model into one category\n",
    "df_train['platform_bandwidth'] = np.where(\n",
    "        ~df_train['platform_bandwidth'].isin(bottom_band),\n",
    "        df_train['platform_bandwidth'],\n",
    "        'Other')\n",
    "\n",
    "df_test['platform_bandwidth'] = np.where(\n",
    "        ~df_test['platform_bandwidth'].isin(bottom_band),\n",
    "        df_test['platform_bandwidth'],\n",
    "        'Other')\n",
    "\n",
    "# Dummy-encode categorical columns\n",
    "df_train = pd.get_dummies(\n",
    "        df_train, \n",
    "        columns=set(df.select_dtypes(include=['object']).columns) - \n",
    "        set(['auction_id', 'zip', 'bid_floor']))\n",
    "\n",
    "df_test = pd.get_dummies(\n",
    "        df_test, \n",
    "        columns=set(df.select_dtypes(include=['object']).columns) - \n",
    "        set(['auction_id', 'zip', 'bid_floor']))\n",
    "\n",
    "\n",
    "df_train.to_pickle('df_train.pickle')\n",
    "df_test.to_pickle('df_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
